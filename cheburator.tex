\documentclass{article} 
\usepackage{amssymb, amsfonts, amsmath, amsthm}
\usepackage{epsfig,lpic,wrapfig}
\usepackage[T2A,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage{url}
%\usepackage[top=0.9in, bottom=0.9in,left=0.9in, right=0.9in, paperwidth=6in, paperheight=9in]{geometry}
\usepackage[colorlinks=true,
citecolor=black,
linkcolor=black,
anchorcolor=black,
filecolor=black,
menucolor=black,
urlcolor=black%
]{hyperref}
\hypersetup{pdftitle={О роботе чебураторе.},
pdfauthor={Александр Гиль}}



\begin{document}
\title{О роботе чебураторе}
\author{Александр Гиль}
\date{}
\maketitle
\begin{abstract}
Две лекции прочитанные в кружке ??? 
\end{abstract}

\section{Сумма геометрической прогрессии и предельные вероятности}

Для вычислений, которыми мы будем заниматься сегодня, 
нам понадобится формула суммирования элементов геометрической прогрессии с первым членом $1$ и знаменателем $x$. 
Мы будем искать способ вычисления суммы 
\[1+x+x^2+\dots+x^n\] 
за небольшое фиксированное количество действий. 
Например, если $x=1$, каждое из  слагаемых нашей суммы равно $1$, 
и, 
таким образом, 
мы можем заменить сложение  равных слагаемых умножением, получив результат $n+1=(n+1)\cdot 1$. 
Если же $x\ne 1$, 
все слагаемые --- разные, и нам придётся воспользоваться каким-то трюком для упрощения вычисления суммы. 
Обратим внимание, что при умножении суммы на $x$ мы получаем очень похожую сумму 
(так что может подойти приём под названием \emph{телескопическая сумма} --- сумма складывается, как раздвижная подзорная труба). 
Давайте отнимем от нашей суммы её же, помноженную на $x$ 
(то есть, умножим нашу сумму на $1-x$) 
и приведём подобные слагаемые: 
\begin{align*}
 (1-x)\cdot(1+x+x^2+\dots+x^n)&=1-x+x-x^2+x^2-\dots-x^{n+1}=
 \\
 &=1-x^{n+1}.
\end{align*}
Поделив обе части равенства на $1-x$ 
(на это число можно делить: 
оно не равно нолю, потому что мы рассматриваем случай $x\ne1$) 
мы получаем искомую формулу:
\[1+x+x^2+\dots+x^n=\frac{1-x^{n+1}}{1-x}.\]

В задачах, которые мы будем решать с использованием этой формулы, идёт речь о вероятностях --- вещественных числах в промежутке от $0$ до $1$. Давайте разберёмся, что происходит с результатом нашей формулы в случае, когда $0<x<1$, 
а число $n$ (количество степеней $x$ в нашей сумме) 
мы хотели бы сделать каким угодно большим 
(мы хотим \emph{устремить $n$ к бесконечности}, 
в математике это намерение записывают так: $n\to\infty$). 
Единственное место в формуле, где используется $n$ --- это показатель степени $n$ в числителе. 
Заметим, что если $0<x<1$, 
то, когда мы умножаем какое-то число на $x$, 
мы уменьшаем его в $\tfrac1x$ раз 
(а это число $\tfrac1x$, обратное $x$, больше единицы). 
Повторяя операцию уменьшения в $1/x$ раз снова и снова (то есть умножая исходное число на возрастающие степени $x$) мы можем уменьшить его (приблизить к нолю) так сильно, как захотим! 
Таким образом, «при $n\to\infty$» («при $n$ стремящемся к бесконечности») число $x^n$ \emph{стремится к нолю} и результат нашей формулы становится всё ближе к $\tfrac{1-0}{1-x}$, то есть к $\tfrac{1}{1-x}$. 

Чуть подробнее --- давайте рассмотрим разницу
\[\frac1{1-x}-(1+x+\dots+x^n)=\frac1{1-x}-\frac{1-x^{n+1}}{1-x}=\frac{x^{n+1}}{1-x}.\]
--- здесь нам становится ясно, 
что сумма нашей геометрической прогрессии \emph{приближается снизу} к значению $\tfrac{1}{1-x}$, 
никогда не достигая этого значения, но приближаясь к нему (при возрастании $n$) как угодно близко 
(поскольку числитель в $\frac{x^{n+1}}{1-x}$ может при увеличении $n$ быть как угодно близким к $0$.
В такой ситуации говорят, что $\tfrac{1}{1-x}$ --- \emph{предельное значение} или, \emph{предел} 
последовательности сумм вида $1+x+\dot+x^n$ 
расположенных в порядке возрастания $n$.
Каждая новая сумма в этой последовательности сумм в точности повторяет предыдущую сумму, добавляя к ней в конце новое слагаемое (новый член геометрической прогрессии). Представим себе, что мы выписали сумму «всех» членов бесконечной геометрической прогрессии
$1+x+x^2+\dots.$
(такую бесконечную сумму называют \emph{бесконечным рядом} или просто \emph{рядом}). 
Рассуждения, которые мы провели для конечных сумм (\emph{частичных сумм} нашего бесконечного ряда), 
изучая, что с ними происходит по мере удлинения прогрессии, дают нам возможность «забыть про $n$» и сказать: в нашем случае сумма бесконечного ряда (сумма бесконечной геометрической прогрессии с первым членом $1$ и положительным знаменателем $x$, меньшим единицы) равна $\frac{1}{1-x}$:
\[1+x+x^2+\dots=\frac1{1-x}\ \ \text{при}\ 0<x<1.\]

Например, 
построив первую милю моста, 
а потом половину оставшейся мили, 
а потом половину от этого, и так далее, 
и так далее, 
через бесконечное время мы достроим полный двухмильный мост! 
(в этом фантазийном примере мы взяли $x=\tfrac12$ 
и в результате бесконечного суммирования получили результат $\tfrac1{1-1/2}=\tfrac1{1/2}=2$).
«Забыв про $n$» (используя обороты \emph{бесконечный ряд}, \emph{сумма бесконечного ряда}) мы можем проще сформулировать строгое математическое рассуждение о поведении сумм нашей геометрической прогрессии при добавлении в её конец всё новых членов. 

Давайте попробуем порассуждать на этом языке, не забывая при этом, что мы рассуждаем о том, к чему стремятся последовательности частичных сумм бесконечного ряда --- просто заменяя длинные формулировки короткими (про \emph{суммирование бесконечного ряда}). 
Кому-то этот язык может показаться фантастическим (как в истории про бесконечное строительство двухмильного моста), однако, при соблюдении необходимых условий эти рассуждения остаются математически точными рассуждениями!

Предположим, какая-то теорема, результатом которой мы можем воспользоваться, говорит нам, что у последовательности частичных сумм бесконечного ряда $1+x+x^2+\dots$ при $)<x<1$ существует предел (то есть \emph{ряд сходится}, у него есть сумма, конкретное число --- \emph{предельное значение последовательности частичных сумм}). Всё, что нас просят теперь --- воспользовавшись теоремой, найти формулу этой суммы. 
Обозначим это искомое число $S$;
то есть
\[S=1+x+x^2+\dots.\] 
Вынесем в правой части этого равенства $x$ за скобки
\[S=1+x\cdot(1+x+x^2+\dots).\] 
--- обратите внимание, в скобках --- ровно та же формула, которую мы обозначили величиной $S$ 
Давайте сделаем соответствующую подстановку и решим полученное уравнение относительно $S$:
\begin{align*}
S&=1+x\cdot S,
\\
S-x\cdot S=1,
\\
(1-x)\cdot S=1,
\\
S=\frac1{1-x}.
\end{align*}
--- то есть, разговаривая на новом языке про суммы бесконечных рядов мы быстрее и проще получили корректную формулу предела сумм геометрической прогрессии с первым членом $1$ и со знаменателем $x$ между $0$ и $1$!

Однако, используя такую удобную возможность сократить усилия, 
следует быть осторожными и чётко помнить о предварительных условиях (условиях упомянутой теоремы, которая открыла эту возможность) 
--- в данном примере важно помнить, 
что $0<x<1$ 
--- ведь если $x\ge 1$, то предельного значения последовательности частичных сумм (то есть, нашей величины $S$) не существует, так что формула $s=\tfrac1{1-x}$ перестаёт быть верной.
Давайте для закрепления рассмотренных приёмов разберём задачу, в которой мы определим численные значения вероятностей для некоторых неограниченных последовательностей испытаний (такие предельные значения вероятностей в похожих задачах из реального мира могут иметь большое практическое значение).

\noindent\textbf{Задача о Чебураторе.} 
Электромеханическая игрушка «Робот Чебуратор», 
будучи включенной, через равные интервалы времени делает шаги одинаковой длины налево или направо, с одинаковой вероятностью $\tfrac12$ выбирая между этими двумя направлениями. 
Чебуратора поставили на правый край стола шириной в один шаг, так что он может шагнуть влево и остаться на столе, в то время как шагая вправо он чебурахнется со стола. Какова вероятность того, что он чебурахнется с правого края стола, и какова --- что с левого?

Рассмотрим первый вопрос задачи. 
Во-первых, Чебуратор чебурахнется направо на первом шагу с вероятностью $\tfrac12$. 
В противном случае (с вероятностью тоже $\tfrac12$) он переместится на левый край стола и будет иметь два равновероятных варианта второго шага. Таким образом, на втором шаге с вероятностью $\tfrac12\cdot\tfrac12=\tfrac14$ он чебурахнется налево, 
и с такой же вероятностью  $\tfrac14$ он вернётся к исходной позиции на правом краю стола. 
На третьем шаге (находясь на правом крае стола) он чебурахнется направо с вероятностью $\tfrac14\cdot\tfrac12$; 
с такой же вероятностью он шагнёт на левый край. 
На четвёртом шаге он вернётся на правый край с вероятностью $\tfrac14\cdot\tfrac12\cdot\tfrac12=(\tfrac14)^2$, после чего чебурахнется направо на пятом шаге с вероятностью $(\tfrac14)^2\cdot\tfrac12$. 
Ситуация повторяется, и мы легко можем доказать (по индукции), что Чебуратор чебурахнется направо на шаге под номером $2{\cdot}n+1$ 
с вероятностью $(\tfrac14)^n\cdot\tfrac12$. 
Эти числа образуют геометрическую прогрессию со знаменателем $\tfrac14$. Суммируя по полученной выше формуле суммы геометрической прогрессии, получим вероятность чебураханья направо на -ом шаге или раньше:
\[
P_{2\cdot n+1}
=\frac12(1+\tfrac14+(\tfrac14)^2+\dots+(\tfrac14)^n)
=\frac12\cdot\frac{1-(\frac14)^{n+1}}{1-\frac14}
=\frac23\cdot\left(1-(\tfrac14)^{n+1}\right).\]

При неограниченном увеличении числа $n$ наша вероятность становится всё ближе к числу $\tfrac23$, никогда не превосходя это число. Таким образом, ответить на вопрос задачи можно так: предельная вероятность чебураханья Чебуратора с правого края стола равна $\tfrac23$. 

Заметим, что в условии задачи формулировка («из жизни») была нечёткая (что означает там слово «вероятность», когда последовательность испытаний не ограничена?), но, решая задачу, мы добились ясности вопроса. Вы заметили, что мы только что на этом конкретном примере рассматривали поведение частичных сумм бесконечной геометрической прогрессии? 
Пользуясь выражением \emph{сумма бесконечной геометрической прогрессии}, 
мы бы быстрее получили то же самое значение предельной вероятности (обозначим это значение $P$):
\[P=\tfrac12\cdot(1+\tfrac14+(\tfrac14)^2+\dots)=\frac12\cdot\frac1{1-\frac14}=\frac12\cdot\frac43=\frac23.\]

Отвечая на второй вопрос задачи (про предельную вероятность чебураханья Чебуратора с левого края стола), мы можем провести такую же подробную цепочку вычислений. 
Всё начнётся с необходимого условия первого шага в левую сторону, вероятность которого $\tfrac12$. 
Давайте на ситуацию после этого шага (в которой Чебуратор стоит на левом краю стола) посмотрим через зеркало, 
которое меняет левое с правым. 
Поскольку вероятности шагов влево и вправо (каждая из которых $\tfrac12$) равны, то, что мы увидим в зеркало, никак не отличается от первой части задачи! 
Таким образом, мы бы повторили те же самые вычисления (но домножив результат на $\tfrac12$ --- вероятность первого шага влево), получив в ответе $\tfrac12\cdot\tfrac23=\tfrac13$.

Получив ответы на вопросы в задаче, мы можем также ответить интересный вопрос, который не был сформулирован в условии: какова предельная вероятность того, что Чебуратор никогда не упадёт со стола? 
Ответ будет таким же, как в таком вопросе про неограниченно долгую серию подкидываний монеты: 
какова вероятность того, что монета неограниченно долго будем падать попеременно орлом и решкой (нечётные броски --- орлом, чётные --- решкой)? 
Легко понять, что вероятность сохранения интересующей нас ситуации после шага $n$ равна $(\tfrac12)^n$. 
С увеличением $n$ эта величина приближается сколь угодно близко к нолю, поэтому ответ на наш вопрос о этой предельной вероятности (третей по счёту из рассматриваемых нами) --- $0$. 
Интересно, что четвёртого варианта результата неограниченно продолжаемой последовательности испытаний из нашей задачи нет 
--- Чебуратор или чебурахается с правого края стола, 
или с левого, 
или неограниченно остаётся на столе. 
Мы определили все три предельных вероятности, и их сумма ($\tfrac23+\tfrac13+0=1$) оказалась равна $1$, 
как и в ограниченных сериях испытаний! 
Так оно и должно быть, потому что этот инвариант 
(сумма вероятностей трёх разных исходов после шага $n$ равна $1$), верный на каждом шаге, остаётся верен и при \emph{предельном переходе} («при $n\to\infty$», то есть «при $n$ стремящемся к бесконечности»). 
Заметив это, мы могли бы ответить на третий вопрос, просто отняв от $1$ сумму ответов на первый и второй вопрос ($1-\tfrac23-\tfrac13=0$).

Если существование предельной вероятности какого-то события в неограниченной последовательности элементарных шагов каким-то образом обосновано (в задачах нашего кружка по этой теме такое обоснование предоставлять не требуется), численное значение этой предельной вероятности иногда можно определить более простыми рассуждениями. Продемонстрируем такие рассуждения для второй части задачи о Чебураторе, определив предельную вероятность его чебураханья с левого края стола. Обозначим искомое число $P$. 
Вычисляя $P$ мы сначала определяем вероятности того, что Чебуратор чебурахнется налево на втором шаге (это $\tfrac14$) 
и того, что на втором шаге он вернётся в исходное состояние (тоже $\tfrac14$). 
Вероятность чебураханья налево из исходного состояния после второго шага --- тоже  поскольку, если Чебуратор остался на столе после первых двух шагов, дальнейшая неограниченная последовательность шагов ничем не отличается от исходной неограниченной последовательности шагов! Таким образом,
\begin{align*}
P&=\tfrac14+\tfrac14\cdot P,
\\
\tfrac34\cdot P&=\tfrac14,
\\
P=\tfrac13.
\end{align*}

Наш результат оказался таким же, как и при решении суммированием геометрической прогрессии! 
Вы заметили, что мы воспользовались тем же приёмом «если нам дано, что искомое \emph{предельное значение} существует, 
мы легко его находим, решая уравнение», с помощью которого мы легче и короче нашли сумму бесконечной геометрической прогрессии с первым членом $1$ и со знаменателем между $0$ и $1$?

\section{Математические ожидания и марковские цепи}

На этом занятии мы рассмотрим ряд тем в продолжение предыдущего занятия («Сумма геометрической прогрессии и предельные вероятности»), разбирая примеры и решая задачи. 
Первое важное, часто встречающееся и интуитивное понятие, которое мы рассмотрим, называется \emph{математическое ожидание} случайной величины или просто \emph{среднее}. 
Оно возникает, когда для каждого испытания в серии одинаковых испытаний определяется некое число (это число, обусловленное результатом испытания, и есть только что упомянутая \emph{случайная величина}) --- например, количество очков, выпадающее при броске игрового кубика (или «игральной кости»), на $6$ гранях которого нанесены пометки-«очки» точками, от одной до шести точек). 
Если кубик --- «честный», 
то есть каждая из шести граней выпадает при броске кубика с одинаковой вероятностью ($\tfrac16$), для вычисления среднего количества очков мы должны вычислить взвешенную сумму значений нашей величины для каждого исхода, взяв вероятность каждого исхода в качестве веса (важно, что сумма весов равна $1$ --- мы сделаем ошибку, если не учтём какой-то исход, вероятность которого не нулевая!). 
В нашем примере все $6$ весов равны $\tfrac16$, так что искомое математическое ожидание выпавшего количества очков равно
\[1\cdot\tfrac16+2\cdot\tfrac16+3\cdot\tfrac16+4\cdot\tfrac16+5\cdot\tfrac16+6\cdot\tfrac16=3\tfrac12.\]
--- среднему арифметическому равновероятных значений нашей величины для каждого из исходов испытания. 
В более сложных случаях вероятности исходов могут быть различны, тем не менее, термин «среднее» остаётся в силе --- так как математическое ожидание (если оно существует в данной схеме испытаний) равно пределу среднего арифметического значений случайной величины в серии испытаний, по мере удлинения серии до бесконечности. 
Применяя этот подход к примеру с игровым кубиком, мы будем раз за разом подкидывать кубик, записывать выпавшее число (целое от $1$ до $6$) и вычислять среднее арифметическое всех записанных чисел. 
При увеличении количества испытаний $n$ 
количества записей каждого 
из $6$ равновероятных исходов среди всех записанных чисел 
будут всё ближе к $\tfrac{n}6$, 
и вычисленные средние нашей случайной величины будут всё ближе приближаться к теоретически вычисленному значению математического ожидания ($3\tfrac12$). 
Теперь, когда мы рассмотрели связи математического ожидания со средним арифметическим, мы для краткости будем называть математическое ожидание случайной величины \emph{средним значением} этой величины или просто \emph{средним}.

Может оказаться, что среднее (математическое ожидание), вычисляемое как предел для продлеваемой бесконечно серии испытаний, не существует для данной схемы испытаний (ведь у бесконечной последовательности чисел предельного значения может не существовать!). Популярным примером такой ситуации является так называемый «Санкт-Петербургский парадокс», в котором рассматривается азартная игра с повторяющимся подкидыванием монетки. 
Раунд игры кончается, как только при очередном подкидывании выпадает орёл, и игрок получает выигрыш в размере $2^n$ дукатов 
(где $n$ --- номер первого подкидывания с орлом), 
т.е. $2$ дукатa если орёл выпадет при первом подкидывании; 
$4=2^2$ дуката, если при первом подкидывании выпадет решка, но при втором выпадет орёл; 
$8=2^3$ дуката, 
если при первом и втором подкидывании выпадет решка, 
но при третьем выпадет орёл, 
и так далее. 
Какую максимальную цену следует платить игроку за раунд участия в этой игре? 
Для ответа на такой вопрос вычисляют средний выигрыш; 
в данном случае это сумма следующего ряда 
(в котором слагаемое под номером $n$ --- это вероятность окончания раунда на $n$-ом подкидывании монеты, умноженная на соответствующий выигрыш):
\[\tfrac12\cdot2+(\tfrac12)^2\cdot4+(\tfrac12)^3\cdot8+\dots=1+1+1+\dots=\infty.\]
То есть средний выйгрыш в такую игру равен бесконечности.
Это приводит нас к парадоксальному выводу: игроку всегда выгодно покупать право на участие в этой игре, какой бы дорогой ни была фиксированная цена за раунд!

В наших задачах такого расхождения рядов происходить не будет (но мы не будем строго доказывать сходимость). Заметим, что практический вывод о том, что игроку следует платить любую сколь угодно дорогую цену за билет на один раунд (поскольку, играя много раундов, он в дальней перспективе выиграет больше) сомнителен из-за ограничений реального мира (реальный игрок не может себе позволить играть слишком долго --- если среднее время ожидания нужного ему выигрыша неприемлемо велико, ему не следует вступать в игру); в других способах разрешить этот парадокс используются различные реалистичные модели мотивации игрока.


Давайте снова рассмотрим серию подкидываний монеты --- но теперь вместо выигрыша игрока мы будем рассчитывать среднюю длину последовательностей решек, выпавших подряд. Такая последовательность ограничена со стороны своего начала или началом серии подбрасываний монетки, или орлом, выпавшим перед первой решкой последовательности. Со стороны конца она ограничена орлом, выпадение которого завершает эту последовательность решек (заметим, что конца серии нет --- чтобы получить среднее в пределе, мы рассматриваем неограниченно длинные серии подбрасываний монетки, как в Санкт-Петербургском парадоксе). Отсортируем выписанные последовательности решек по длине и посчитаем среднюю длину, взяв длины выписанных последовательностей, помноженные каждая на свой «вес» --- предельное значение доли последовательностей такой длины среди общего количества выписанных последовательностей при стремлении длины серии подкидываний к бесконечности. 
У всех последовательностей есть одна решка в начале, и (в пределе) половина последовательностей на этом и кончаются 
(поскольку при следующем подкидывании монетки после первоначальной решки с вероятностью $\tfrac12$  выпадает орёл). 
Половина из оставшихся последовательностей (то есть $\tfrac14$ от общего количества последовательностей) имеет длину 2;
$\tfrac18$ последовательностей имеет длину 3, 
$\tfrac116$ --- длину 4 
--- и так далее. 
В результате мы можем рассчитать интересующую нас среднюю длину как сумму такого бесконечного ряда (похожего на ряд из Санкт-Петербургского парадокса --- но с другими значениями «выигрыша», обеспечивающими в этот раз сходимость ряда):
\[S=(\tfrac12)\cdot 1+(\tfrac12)^2\cdot 2+(\tfrac12)^3\cdot 3+\dots\]
Ряд этот сходится благодаря тому, что показательная функция --- вес $n$-го слагаемого --- убывает значительно быстрее линейной функции (значения усредняемой случайной величины, длины последовательности решек); мы не будем сейчас строго это доказывать. 
Опираясь на постулированную сходимость ряда, найдём его сумму с помощью уравнения, прежде всего выделив из суммируемого ряда сумму геометрической прогрессии весов и воспользовавшись формулой с прошлого занятия:
\[\tfrac12+(\tfrac12)^2+(\tfrac12)^3+\dots=\frac1{1-\frac12}-1=2-1=1\]

Давайте отнимем поэлементно ряд в левой части этого равенства от правой части определения $S$ а сумму ряда (то есть число 1) от левой части определения $S$ (то есть от $S$). 
В правой части после вынесения множителя $\tfrac12$ за скобки мы узнаем  и, решив уравнение, сможем найти значение этой величины:
\begin{align*}
S
=&(\tfrac12)+
\\
+&(\tfrac12)^2+(\tfrac12)^2+
\\
+&(\tfrac12)^3+(\tfrac12)^3+(\tfrac12)^3+
\\
+&\cdots
\end{align*}
Суммируя столбцы получаем
\[S=1+\tfrac12+(\tfrac12)^2+\dots=2.\]
Итак, мы доказали, что средняя длина непрерывной последовательности решек равна двум. 

Каково среднее количества подкидываний монеты от начала подкидываний до того, как орёл выпадает в первый раз? 
Просуммировав ряд, мы придём к ответу «один», 
но нам хотелось бы выразить это число в виде формулы от вычисленной средней длины последовательности решек (нашего $S$, которое равно $2$). 

Давайте проверим нашу вероятностную интуицию. 
С одной стороны, с началом подкидываний монеты мы можем равновероятно попасть на любое место отрезка из решек, 
так что среднее времени ожидания орла должно быть $S/2$ 
(примерно как, придя на остановку автобуса в случайный момент, мы в среднем ждём половину временного интервала между последовательными рейсами этого автобусного маршрута в это время суток). 
С другой стороны, начало непрерывной последовательности решек отличается от начала серий подкидываний монеты тем, что первое подкидывание в непрерывной последовательности решек --- обязательно решка, а последовательность начиная со второго подкидывания ничем не отличаются от любого начала подкидываний, так что среднее время ожидания орла равно $S-1$. 
Обе формулы ($S/2$ и $S-1$ дают верный результат 
(1 --- вспомним, что одно из равенств при нахождении $S$ было $S-1=\tfrac12\cdot S$), 
но какое из двух интуитивных объяснений справедливо, а какое --- нет? 

Чтобы разобраться в этом вопросе, рассмотрим серию событий (таких, как подкидываний монеты или бросков игрального кубика), 
в которых вероятность интересующего нас исхода (падения монеты орлом кверху или выпадения грани с одной точкой на игральном кубике) --- не $\tfrac12$ (как при подкидывании монеты), 
а какая-то произвольная вероятность $p$
--- например, для выпадения единицы на игральном кубике $p=\tfrac16$ а на «игральном октаэдре» (они используются в некоторых настольных играх) $p=\tfrac18$. 
Эта обобщённая схема простых случайных испытаний называется «испытания Бернулли». 
Давайте найдём и решим уравнение для интересующего нас среднего $S$.
Проведя длинную серию испытаний (записанную как последовательность чисел-исходов), выпишем в столбик все непрерывные под-последовательности (ненулевой длины!), в которых интересное нам событие не происходило, отсортировав их по мере увеличения длины. 
Доля последовательностей длины $1$ в пределе будет равна $p$ потому что такова вероятность наступления интересного нам события (например, выпадения одного очка на игральном кубике) после первого элемента в выписанных последовательностях: именно это событие обрывает последовательность.
Доля всех более длинных последовательностей будет равна $1-p$; 
далее мы сделаем уже знакомые расчёты (с той разницей, что некоторые $\tfrac12$-е заменены на $p$, а некоторые --- на $1-p$):

\begin{align*}
S&=p\cdot 1+(1-p)\cdot p\cdot 2+(1-p)^2\cdot p\cdot 3+\dots=
\\
&+p\cdot(1+(1-p)\cdot 2+(1-p)^2\cdot 3+\dots)=
\\
&=p(1+(1-p)+(1-p)^2+\dots)+
\\
&\ \ +(1-p)\cdot(p\cdot 1+(1-p)\cdot p\cdot 2+(1-p)^2\cdot p\cdot 3+\dots);
\\
S&=1+(1-p)\cdot S;
\\
p\cdot S&=1;
\\
S&=\tfrac1p.
\end{align*}



В примере с кубиком ($p=\tfrac16$) 
в результате имеем $6$ --- средняя длина отрезка испытаний между наступлением события с вероятностью $p$ равна числу $\tfrac1p$.
Таким образом, проанализировав схему испытаний Бернулли, мы заново открыли одну из форм следующего закона: средний интервал времени между наступлением некоего случайного события (такого, как выпадение конкретной грани игрального кубика) есть число, обратное частоте события (среднего количества таких событий за единицу времени). 
Рассчитывая среднее ожидание события от начала испытаний (назовём его $P$), 
мы проделаем похожие вычисления (мы можем сократить их, при первой возможности выразив $P$ через $S$):
\[P=p\cdot 0+(1-p)\cdot p\cdot 1+(1-p)^2\cdot p\cdot 2+\dots.\]
\[P=(1-p)\cdot S=\frac{1-p}{p}=\tfrac1p-1=S-1.\]

Мы пришли к выводу, что правильным был второй интуитивный подход (утверждавший, что $P=S-1$), 
а не первый (утверждавший, что $P=S/2$). 
Первый подход мог бы быть верен, 
если бы мы поставили наш эксперимент по схеме «автобусной остановки» 
с фиксированным расписанием исходов испытания 
(например, $1,2,3,4,5,6,1,2,\dots$ по кругу для чисел от $1$ до $6$), 
да ещё дополнительно разрешили начинать отсчёт ожидания на дробных временных интервалах. Как мы выяснили, это не наш случай --- случайные события не ходят по расписанию, хотя и имеют среднее время ожидания! Заметим, что если бы мы были уверены во втором подходе с самого начала, мы могли бы очень быстро найти $S$ и $P$ из двух равенств: с одной стороны,
\[P=S-1\]
(отрезав обязательный первый элемент от выписанных для вычисления $S$ последовательностей, мы получаем серию последовательностей, неотличимую от серий последовательностей в ожидании первого события от начала серии испытаний); с другой же стороны,
\[P=p\cdot0+(1-p)\cdot S\]

(когда интересное событие наступает первым после начала с вероятностью $p$, 
это даёт вклад $0$ в вычисление среднего, 
а в противном случае, вероятность которого $1-p$ вклад равен $S$). 
Итого
\[P=S-1=(1-p)\cdot S; \ p\cdot S=1; S=\tfrac1p; P=\tfrac1p-1.\]

Натренировавшись на испытаниях Бернулли, посчитаем среднее в более сложной схеме --- например, в опытах с Чебуратором из первого занятия. 
На каком в среднем номере шага он чебурахается со стола? 
Мы можем быстро посчитать это число, воспользовавшись симметрией (лево / право); назовём искомое среднее $Q$. 
Заметим, что с вероятностью $\tfrac12$ Чебуратор чебурахается на первом шаге направо, а с оставшейся вероятностью $\tfrac12$ он, 
сделав один шаг (не забудем прибавить эту единицу!), 
попадает в ситуацию, абсолютно симметричную начальной:
\begin{align*}
Q&=\tfrac12\cdot1+\tfrac12\cdot(1+Q);
\\
Q&=1+\tfrac12\cdot Q;
\\
\tfrac12\cdot Q&=1;
\\
Q&=2.
\end{align*}

Это было легко! Давайте рассмотрим задачу посложнее, с двумя параметрами.

\noindent\textbf{Задача про лягушку:} 
лягушка сидит на одной из двух кочек, и через каждую секунду она может или перепрыгнуть на другую кочку, или остаться на месте. 
При этом с первой кочки на вторую она прыгает с вероятностью $p$ а со второй на первую --- с вероятностью $q$. 
Если мы будем следить за лягушкой достаточно долго, какую долю времени в среднем она проведёт на первой кочке, и какую --- на второй?

Обозначив первое искомое число $P$ сразу заметим, 
что второе искомое число (которое мы обозначим $Q$) 
равно $1-P$ 
(у лягушки только два места, а временем полётов в прыжках мы пренебрегаем). 
Подсчитав $Q$ другим способом, решим уравнение и найдём  $P$.

Давайте считать, что мы записываем наблюдения за положением лягушкой в каждую секунду (записываем цифру: единицу, если в данную секунду лягушка на первой кочке, или двойку, если она на второй кочке). Для удобства рассмотрения выкинем начало записи, если мы начали наблюдать за лягушкой, когда она сидела на второй кочке: дождёмся в этом случае момента, когда она в первый раз перепрыгнет на первую кочку, и вот с этой секунды начнём записывать (так что наша последовательности начинается с единицы); поскольку вести наблюдения мы будем достаточно долго, отбрасывание стартовых двоек вычисляемые средние не изменит. Теперь каждой двойке предшествует или единица (назовём такую двойку «двойкой первого типа»), или другая двойка (тогда назовём нашу двойку «двойкой второго типа»), мы посчитаем доли двоек первого и второго типов отдельно и затем сложим, чтобы получить искомое $Q$. 
Доля двоек первого типа среди всех цифр равна $p\cdot P$ --- ведь доля единиц среди всех цифр --- это $P$, а после единицы следует двойка с вероятностью $p$. 
Доля же двоек второго типа среди всех цифр равна $(1-q)\cdot Q$ --- ведь доля двоек среди всех цифр --- это $Q$, 
а за двойкой идёт опять двойка с вероятностью $1-q$. 
Получаем уравнение с неизвестными $P$ и $Q$:
\[Q=p\cdot P+(1-q)\cdot Q.\]
Подставим $Q=1-P$ и решим относительно $P$ 
(напоследок выписав формулу и для $Q$):
\begin{align*}
1-P&=p\cdot P+(1-q)\cdot (1-P);
\\
1-P&=p\cdot P+1-q-P+q\cdot P;
\\
q&=(p+q)\cdot P;
\\
P&=\frac{q}{p+q};
\\
Q=1-P=\frac{q}{p+q}.
\end{align*}

Как мы только что выяснили, предельные вероятностей нахождения нашей системы «лягушка на кочках» в одном из двух состояний полностью определяется вероятностями переходов между состояниями. Обобщённое название таких систем (в которых вероятности переходов между состояниями в конкретный момент не зависят от истории прошлых переходов) --- \emph{марковские цепи}, 
и мы только что рассмотрели пример так называемой \emph{неприводимой} марковской цепи. 
Она отличается от марковских цепей, описывающих задачи о Чебураторе и задачу про пьяницу, в других наших задачах система за конечное число шагов приходит в одно из терминальных состояний и остаётся в нём.

Напоследок рассмотрим ещё одну задачу с большим количеством состояний системы, всегда завершающей свою эволюцию за конечное число шагов (подобно системам в задачах о Чебураторе). Нам потребуется больше вычислений, но теперь это нас не испугает!

\noindent\textbf{Задача о пьянице, возвращающемся домой:} 
пьяница вышел из бара, расположенного в  кварталах от своего дома на той же улице, чтобы вернуться домой; чтобы сделать дорогу домой веселее, он разнообразит её на каждом перекрёстке следующим образом. 
Дойдя до перекрёстка, он подкидывает монетку --- и, если она выпадает орлом, он продолжает путь в том же направлении; если же она выпадает решкой, он разворачивается и идёт в противоположном направлении. Если ему случается вернуться к бару, он всегда разворачивается в сторону дома; если же он дошёл до своего дома, он завершает свой путь. Сколько кварталов в среднем он пройдёт на этом пути?

Для решения этой задачи удобно ввести переменные таким образом, чтобы для каждого перекрёстка мы бы решали более простую подзадачу. 
Пронумеруем перекрёстки их расстоянием в кварталах от бара, и обозначим $x_i$  среднюю длину пути, который пьяница, находящийся на перекрёстке $i$, проходит, прежде чем оказаться на перекрёстке $i+1$. 
По условию задачи $x_0=1$ 
(находясь у бара, пьяница всегда идёт в верном направлении, и проходит ровно один квартал до следующего перекрёстка).
Для других же значений $i$ ($0<i<n$) 
формула вычисления  имеет два слагаемых:
\begin{itemize}
\item с вероятностью $\tfrac12$ пьяница идёт в интересующем нас (верном) направлении к перекрёстку $i+1$ 
(в этом случае длина его пути равна одному кварталу);
\item с вероятностью $\tfrac12$ пьяница идёт в другом (неверном) направлении. После того, как он проделает путь в один квартал, пьянице на перекрёстке $i-1$ понадобится в среднем пройти $x_{i-1}$ кварталов обратно к перекрёстку $i$, и затем ещё $x_i$, 
чтобы добраться до перекрёстка $i+1$.
\end{itemize}
Это рассмотрение приводит нас к следующему уравнению, которое мы решим относительно $x_i$:
\begin{align*}
x_i&=\tfrac12\cdot 1+\tfrac12\cdot(1+x_{i-1}+x_i);
\\
x_i&=2+x_{i-1}.
\end{align*}

Теперь, отталкиваясь от $x_0=1$, 
мы получаем $x_1=2+x_0=3$, 
затем $x_2=2+x_1=5$  --- и так далее (то есть $x_i=2\cdot i+1$). 
Ответ задачи (обозначим его $S$) 
равен сумме всех наших $x_i$; найти формулу, выражающую  через  нам поможет формула суммы арифметической прогрессии:
\[S=1+2+\dots+(2\cdot n-1)=n^2\]

Найдя ответ на эту задачу, мы получили интересный факт для теории марковских цепей специального вида, называемых случайными блужданиями (в данном случае --- в одном измерении, на линии) --- как мы выяснили, чтобы удалиться от начала блужданий (бара в нашей задаче) на некоторое заданное расстояние, пьяница в среднем должен потратить время, пропорциональное квадрату этого расстояния. 

Похожие вероятностно-статистические вычисления провёл Альберт Эйнштейн в статье 1905 года, объяснявшей броуновское движения микроскопических частиц в жидкости тем, что их подталкивают молекулы жидкости. 
Вычисления Эйнштейна позволили оценить размер молекул (и вообще доказательно подтвердить атомную теорию строения вещества) с помощью обычного микроскопа, за многие десятилетия до того, как появились сверхмощные микроскопы, дающие разрешение на уровне отдельных молекул и атомов. Заметьте, что мы получили этот факт про случайные блуждания, не используя математики сложнее суммирования бесконечной геометрической прогрессии и решения системы уравнений с несколькими переменными (правда, мы не всегда строго доказывали сходимость рядов).

\end{document}
